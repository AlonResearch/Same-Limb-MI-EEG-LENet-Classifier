# Project Structure Requirements Guideline

> **Purpose:** This document provides standards for refactoring monolithic Jupyter notebook applications into well-organized, modular Python projects. Use this guideline when an AI agent needs to transform a single-file notebook into a production-ready codebase.

---

## 1. Directory Structure

All projects **MUST** follow this standard layout, adapted for necessary data stages:

```
project_root/
├── pyproject.toml          # Project metadata, dependencies, tool configs
├── README.md               # Detailed project overview, stages, and instructions
├── data/
│   ├── raw/                # Immutable original data
│   ├── processed/          # Final datasets ready for modeling
│   └── ...                 # Other subfolders for specific stages (e.g., "preprocessed")
├── models/                 # Serialized trained models (.pkl, .joblib, etc.)
├── notebooks/              # Exploratory/demo notebooks (NOT main logic)
├── reports/                # Generated outputs: figures, metrics, HTML reports
├── src/
│   └── <package_name>/     # Main Python package (importable)
│       ├── __init__.py
│       ├── config.py       # Paths, constants, environment settings
│       ├── logger.py       # Centralized logging configuration
│       ├── fetch.py        # Data acquisition (API calls, downloads)
│       ├── dataset.py      # Data loading, cleaning, feature engineering
│       ├── model.py        # Model definition, pipeline construction
│       ├── train.py        # Training orchestration, evaluation, saving
│       ├── ui.py           # Optional: UI/visualization/inference entry
│       └── main.py         # Minimal orchestrator script (Calls key stages)
└── tests/                  # Unit and integration tests
    ├── test_dataset.py
    └── test_model.py
```

### Key Rules

| Folder | Purpose | Rule |
|--------|---------|------|
| `data/` | Data lineage stages | Create subfolders for **every step** (raw, sliced, preprocessed, etc.) |
| `data/raw/` | Original, unmodified data | **Never edit manually**—treat as immutable |
| `data/*/` | Intermediate stages | Generated by code, reproducible, logical naming |
| `models/` | Serialized models | Include version/date in filename if needed |
| `notebooks/` | EDA, demos, prototyping | Should import from `src/`, not contain core logic |
| `reports/` | Output artifacts | Figures, metrics files, HTML reports |
| `src/<pkg>/` | Production code | All importable, reusable code lives here |
| `tests/` | Test suite | Mirror structure of `src/` for discoverability |
| `main.py` | Orchestration | **Minimal code**: Calls large building blocks |

---

## 2. Module Separation by Responsibility

When refactoring a monolithic notebook, split code into modules based on **Single Responsibility Principle**:

### 2.1 `config.py` — Configuration & Paths

```python
"""Project configuration."""
from dataclasses import dataclass
from pathlib import Path

@dataclass(frozen=True)
class Paths:
    """Standard project paths."""
    project_root: Path
    data_raw: Path
    data_processed: Path
    models: Path
    reports: Path

    @staticmethod
    def from_here() -> "Paths":
        root = Path(__file__).resolve().parents[2]
        return Paths(
            project_root=root,
            data_raw=root / "data" / "raw",
            data_processed=root / "data" / "processed",
            models=root / "models",
            reports=root / "reports",
        )

# External API endpoints, constants
API_BASE_URL = "https://api.example.com/v1"
```

**Requirements:**
- Use `dataclass(frozen=True)` for immutable config objects
- Derive paths dynamically from `__file__` for portability
- Centralize all magic strings, URLs, thresholds here

---

### 2.2 `logger.py` — Logging Configuration

```python
"""App logger."""
import logging

logging.basicConfig(
    level=logging.DEBUG,
    format="%(asctime)s - %(filename)s - %(funcName)s - %(levelname)s - %(message)s",
    handlers=[logging.FileHandler("app.log"), logging.StreamHandler()],
)
logger = logging.getLogger()
```

**Requirements:**
- Single logger instance shared across modules
- Log to both file (`app.log`) and console
- Include timestamp, filename, function name in format

---

### 2.3 `fetch.py` — Data Acquisition

Responsible for: API calls, downloads, caching raw data to disk.

```python
"""Fetch data from external sources."""
from pathlib import Path
from my_project.config import Paths
from my_project.logger import logger

def fetch_and_cache(api_key: str, pages: int) -> list[Path]:
    """Fetch data pages and cache to disk."""
    paths = Paths.from_here()
    # ... fetch logic ...
    logger.info("Cached %d pages to %s", pages, paths.data_raw)
    return cached_files

def main() -> None:
    """CLI entrypoint."""
    import argparse
    parser = argparse.ArgumentParser()
    # ... add arguments ...
    
if __name__ == "__main__":
    main()
```

**Requirements:**
- Cache raw responses to `data/raw/` (reproducibility)
- Include CLI with `argparse` for standalone execution
- Log all network operations

---

### 2.4 `dataset.py` — Data Processing & Feature Engineering

Responsible for: loading raw data, cleaning, transformations, feature creation.

```python
"""Build datasets from raw data."""
from dataclasses import dataclass
import pandas as pd

@dataclass(frozen=True)
class Dataset:
    """Dataset bundle."""
    df: pd.DataFrame
    feature_cols: list[str]
    target_col: str

def build_dataset(json_files: list[Path]) -> Dataset:
    """Build cleaned dataset from raw files."""
    # ... processing logic ...
    return Dataset(df=df, feature_cols=feature_cols, target_col=target)

def save_dataset_csv(dataset: Dataset, out_csv: Path) -> None:
    """Save dataset to CSV."""
    out_csv.parent.mkdir(parents=True, exist_ok=True)
    dataset.df.to_csv(out_csv, index=False)
```

**Requirements:**
- Return structured objects (dataclass), not loose DataFrames
- Separate I/O (load/save) from transformation logic
- Document feature columns explicitly

---

### 2.5 `model.py` — Model Definition

Responsible for: model architecture, pipeline construction, inference methods.

```python
"""Model definition."""
from dataclasses import dataclass
from sklearn.pipeline import Pipeline

@dataclass(frozen=True)
class TrainedModel:
    """Bundle for trained model and metadata."""
    pipeline: Pipeline
    feature_cols: list[str]
    target_col: str

    def predict_proba(self, X) -> np.ndarray:
        """Return probabilities."""
        return self.pipeline.predict_proba(X)[:, 1]

def build_pipeline(seed: int) -> Pipeline:
    """Create ML pipeline."""
    return Pipeline(steps=[
        ("impute", SimpleImputer(strategy="median")),
        ("scale", StandardScaler()),
        ("clf", LogisticRegression(random_state=seed)),
    ])
```

**Requirements:**
- Wrap models in dataclass with metadata (feature names, target)
- Expose clean inference API (`predict_proba`, `predict`)
- Keep model construction separate from training

---

### 2.6 `train.py` — Training Orchestration

Responsible for: loading data, splitting, training, evaluation, saving artifacts.

```python
"""Train model from processed data."""
from my_project.config import Paths
from my_project.dataset import build_from_default_cache
from my_project.model import build_pipeline, TrainedModel
from my_project.logger import logger

def main() -> None:
    """CLI entrypoint."""
    paths = Paths.from_here()
    
    # 1. Load data
    dataset = build_from_default_cache()
    
    # 2. Split
    X_train, X_test, y_train, y_test = train_test_split(...)
    
    # 3. Train
    pipeline = build_pipeline(seed=42)
    pipeline.fit(X_train, y_train)
    
    # 4. Evaluate
    logger.info("Accuracy: %.4f", accuracy_score(y_test, y_pred))
    
    # 5. Save artifacts
    save_model(model, paths.models / "model.pkl")
    save_figure(cm_plot, paths.reports / "confusion_matrix.png")

if __name__ == "__main__":
    main()
```

**Requirements:**
- Orchestrate full workflow via CLI
- Save all artifacts to appropriate directories
- Log metrics and paths

---

### 2.7 `main.py` — High-Level Orchestrator

The `main.py` file should be the entry point that ties all major stages together.

```python
"""Main entry point for project execution."""
from my_project.fetch import fetch_and_cache
from my_project.train import train_pipeline
from my_project.ui import run_ui

def main() -> None:
    """Execute major project stages in sequence."""
    # 1. Data Acquisition
    fetch_and_cache(api_key="DEMO_KEY", pages=3)
    
    # 2. Training and Evaluation (includes processing)
    train_pipeline(seed=42)
    
    # 3. Optional: Interface
    run_ui()

if __name__ == "__main__":
    main()
```

**Requirements:**
- **Minimal Code**: Should contain only calls to key functions from other modules.
- **Building Blocks**: Each call represents a major stage leading to the next.
- **Readability**: Acts as a "table of contents" for the project's execution.

---

## 3. `pyproject.toml` Requirements

All project metadata, dependencies, and tool configurations **MUST** be in `pyproject.toml`:

```toml
[build-system]
requires = ["setuptools>=68", "setuptools_scm[toml]>=8"]
build-backend = "setuptools.build_meta"

[project]
name = "my_project"
requires-python = ">=3.12"
version = "1.0.0"
dependencies = [
    "numpy>=2.0",
    "pandas>=2.0",
    "scikit-learn>=1.5",
    "matplotlib>=3.8",
]

[project.optional-dependencies]
dev = ["my_project[ds]", "my_project[lint]", "my_project[test]"]
ds = ["jupyterlab>=4.0"]
lint = ["mypy", "ruff"]
test = ["pytest>=8.0", "pytest-cov>=5.0"]

[tool.setuptools]
package-dir = {"" = "src"}

[tool.setuptools.packages.find]
where = ["src"]

[tool.ruff]
line-length = 120
target-version = "py312"
lint.select = ["ALL"]
lint.ignore = ["COM812", "ISC001"]  # Conflicts with formatter

[tool.ruff.lint.per-file-ignores]
"**/tests/**" = ["S101", "D103"]  # Allow assert, skip docstrings
"**/__init__.py" = ["F401", "D104"]

[tool.ruff.lint.pydocstyle]
convention = "google"

[tool.mypy]
disallow_untyped_defs = true
warn_unused_ignores = true
ignore_missing_imports = true
```

### Dependency Groups

| Group | Purpose |
|-------|---------|
| `dependencies` | Core runtime requirements |
| `dev` | Everything for development |
| `ds` | Data science tools (Jupyter, etc.) |
| `lint` | Code quality (ruff, mypy) |
| `test` | Testing framework |

---

## 4. Code Quality Standards

### 4.1 Type Hints — Required

```python
# ✓ Good
def build_dataset(json_files: list[Path]) -> Dataset:
    ...

# ✗ Bad
def build_dataset(json_files):
    ...
```

### 4.2 Docstrings — Google Style

```python
def fetch_page(api_key: str, page: int) -> dict[str, Any]:
    """Fetch one page from API.

    Args:
        api_key: API authentication key.
        page: Zero-indexed page number.

    Returns:
        JSON response as dictionary.

    Raises:
        RuntimeError: If HTTP status is not 200.
    """
```

### 4.3 Imports — Use `from __future__ import annotations`

```python
from __future__ import annotations  # Always first

from dataclasses import dataclass
from pathlib import Path
from typing import Any

import numpy as np
import pandas as pd

from my_project.config import Paths
from my_project.logger import logger
```

### 4.4 Dataclasses — Use `frozen=True`

```python
@dataclass(frozen=True)
class Dataset:
    """Immutable dataset bundle."""
    df: pd.DataFrame
    feature_cols: list[str]
    target_col: str
```

### 4.5 Naming Conventions & Variables

- **Project Name**: Should reflect purpose (avoid `FinalProject.py`).
- **Consistency**: Use `snake_case` for functions/variables consistently.
- **Avoid Magic Values**: Never pass raw numbers or strings directly into functions.
- **Descriptive Names**: Variable names should be clear and readable.

```python
# ✓ Good
binary_num = 1
name_to_print = "Asteroid-A"
config_dict = {"param": 10}
process_data(binary_num, name_to_print, config_dict)

# ✗ Bad (No context)
process_data(1, "Asteroid-A", {1: 10})
```

### 4.6 Function Design & Length

- **Short Functions**: Functions **MUST NOT** exceed **50 lines of code**.
- **Extract Logic**: If you find yourself writing a comment above a block of code to explain "what this does", extract that block into a separate, well-named function.
- **Single Task**: Each function should do one thing.

### 4.7 Logging vs Print

- **Never use `print()` statements** for application flow or debugging in the final project.
- Always use the centralized `logger` from `logger.py`.
- Use appropriate levels: `DEBUG` for details, `INFO` for stages, `ERROR` for failures.

---

## 5. Testing Requirements

### 5.1 Test Location & Naming

```
tests/
├── test_dataset.py    # Tests for src/my_project/dataset.py
├── test_model.py      # Tests for src/my_project/model.py
└── test_fetch.py      # Tests for src/my_project/fetch.py
```

### 5.2 Test Structure

```python
from __future__ import annotations

import json
from pathlib import Path

import pandas as pd

from my_project.dataset import build_dataset


def test_build_dataset_smoke(tmp_path: Path) -> None:
    """Smoke test: dataset builds from valid JSON."""
    # Arrange
    objs = [{"id": "1", "name": "Test", ...}]
    fp = tmp_path / "page.json"
    fp.write_text(json.dumps({"data": objs}))
    
    # Act
    ds = build_dataset([fp])
    
    # Assert
    assert isinstance(ds.df, pd.DataFrame)
    assert ds.df.shape[0] == 1
    assert ds.target_col in ds.df.columns
```

### 5.3 Required Test Types

| Type | Purpose | Example |
|------|---------|---------|
| Smoke test | Basic functionality works | `test_build_dataset_smoke` |
| Shape test | Output dimensions correct | `assert p.shape == (2,)` |
| Range test | Values in valid bounds | `assert np.all((p >= 0) & (p <= 1))` |
| Type test | Return types correct | `assert isinstance(ds.df, pd.DataFrame)` |

### 5.4 Stage Coverage

Tests must cover **each major stage** of the project:
1.  **Data Import**: Verify data is fetched and saved correctly.
2.  **Data Processing**: Confirm cleaned data meets intended assumptions (e.g., no negative values, correct types).
3.  **Modeling**: Confirm model predicts as expected.
4.  **Assumptions Testing**: Successful execution must confirm that each stage meets its intended assumptions.

---

## 6. README.md Requirements

The `README.md` is the primary documentation. It **MUST** include:

### 6.1 Project Description
- **Main Objectives**: What is the project trying to achieve?
- **Assumptions**: What are the underlying assumptions about the data or domain?
- **Hypothesis**: What are you testing or predicting?

### 6.2 Structure & Stages
- **Folder/Module Structure**: Explanation of the organization, including sub-modules.
- **Key Stages**: Detailed breakdown of the workflow (e.g., data import → pre-processing → modeling → visualization).
- **Stage Descriptions**: Brief explanation of what was done in each step and why.

### 6.3 Configuration & Data
- **Definitions**: Explanation of key parameters and configuration settings in `config.py`.
- **Data Description**: Details about the features, target, and data source.
- **Dataset Link**: Direct link to the source data used.

### 6.4 Instructions & References
- **Run Commands**: Precise shell commands to execute each stage and the full project.
- **References**: Citations for any papers, articles, or libraries used.

---

## 7. Notebook Usage Guidelines

Notebooks in `notebooks/` are for **exploration only**:

### ✓ Appropriate Notebook Use
- Exploratory Data Analysis (EDA)
- Visualizations and demos
- Quick prototyping
- Documentation with inline outputs

### ✗ NOT Appropriate for Notebooks
- Core business logic
- Data processing pipelines
- Model definitions
- Production code

### Notebook Pattern

```python
# Cell 1: Import from package
from my_project.dataset import build_from_default_cache
from my_project.config import Paths

# Cell 2: Load data using package functions
dataset = build_from_default_cache()
df = dataset.df

# Cell 3: Explore (this is notebook-appropriate)
df.describe()
df.hist()
```

---

## 8. Refactoring Checklist for AI Agents

When refactoring a monolithic notebook into a structured project:

### Step 1: Identify Code Categories
- [ ] Configuration (paths, constants, API URLs)
- [ ] Data fetching (API calls, downloads)
- [ ] Data processing (cleaning, feature engineering)
- [ ] Model definition (architecture, pipelines)
- [ ] Training logic (fit, evaluate, save)
- [ ] Visualization/UI code

### Step 2: Create Directory Structure
- [ ] Create `src/<package_name>/` with `__init__.py`
- [ ] Create `data/raw/`, `data/processed/`, `models/`, `reports/`
- [ ] Create `tests/`
- [ ] Create `notebooks/` (move original notebook here)

### Step 3: Extract Modules
- [ ] Extract config → `config.py`
- [ ] Extract logging → `logger.py`
- [ ] Extract data fetching → `fetch.py`
- [ ] Extract data processing → `dataset.py` (including intermediate stages)
- [ ] Extract model code → `model.py`
- [ ] Extract training → `train.py`
- [ ] Create minimal orchestrator → `main.py`

### Step 4: Add Infrastructure
- [ ] Create `pyproject.toml` with dependencies and project name
- [ ] Ensure consistent `snake_case` naming
- [ ] Check function lengths (max 50 lines)
- [ ] Replace all `print()` with `logger.info()` or `logger.debug()`
- [ ] Add type hints to all functions
- [ ] Add Google-style docstrings
- [ ] Create tests for **each major stage**

### Step 5: Validate
- [ ] `pip install -e .` succeeds
- [ ] `python src/<pkg>/main.py` runs the full pipeline
- [ ] `pytest` passes and validates stage assumptions
- [ ] `ruff check .` passes
- [ ] `mypy src/` passes

---

## 9. Anti-Patterns to Avoid

| Anti-Pattern | Problem | Solution |
|--------------|---------|----------|
| Hardcoded paths | Breaks on other machines | Use `Paths.from_here()` |
| Global state | Hard to test | Pass dependencies explicitly |
| No logging | Hard to debug | Use centralized `logger` |
| Loose DataFrames | Unknown columns | Wrap in `Dataset` dataclass |
| Model without metadata | Unknown features | Wrap in `TrainedModel` dataclass |
| Tests in `src/` | Pollutes package | Use separate `tests/` directory |
| `print()` everywhere | No log levels | Use `logger.info()` |
| `from module import *` | Namespace pollution | Import explicitly |

---

## 10. Quick Reference

### Installation

```bash
pip install -e ".[dev]"      # Full development setup
pip install -e ".[ds]"       # Data science only
pip install -e ".[test]"     # Testing only
```

### Common Commands

```bash
# Run module as script
python -m my_project.fetch --help
python -m my_project.train --seed 42

# Testing
pytest
pytest --cov=my_project

# Linting
ruff check .
ruff format .
mypy src/
```

### Import Pattern

```python
# From within the package
from my_project.config import Paths
from my_project.logger import logger
from my_project.dataset import Dataset, build_dataset
from my_project.model import TrainedModel, build_pipeline
```

---

*This guideline is based on Python packaging best practices, scikit-learn conventions, and the Cookiecutter Data Science template.*
